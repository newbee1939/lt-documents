## 「ChatGPT の頭の中」読書メモ

### はじめに

- ChatGPT のベースにあるのは、ニューラルネットという概念
  - 人間の脳の働きを理想化するために、1940 年代に考案されたもの

### 第一部: ChatGPT は何をしているのか、なぜ動くのか

- 実は、一つずつ単語を足しているだけ
- ChatGPT は基本的に、そこまで出力された内容の「順当な続き」を出力しようと試みる
- ここでいう「順当」とは、「億単位のウェブページなどに書かれている内容を見た上で人間が書きそうだと予測される」という意味
  - 「AI の一番の長所としてあげられるのは」という書きかけの文があるとする
  - 人間が書いた億単位の文章を（ウェブページや電子書籍などから）スキャンして、この一文が出現するあらゆるケースを見つけ、どの単語がどのくらいの確率で次に続くかを調べる
  - 「確率」でランク付けされたリストに従って、あとに続く単語を出力しているに過ぎない
  - hide: あまり情報がないデータに関してはどうしても不正確になる->基本的には新しい概念を生み出すことは構造的に不可能では？
- ChatGPT で驚異的なのは、例えば小論文を書くときなどでも、基本的には「ここまでの文を受けて、次に続く単語は何か？」という質問を繰り返し、その度に一つずつ単語を追加しているに過ぎないということだ
- 厳密にいうと、これもまた後述するように、実際には「トークン」と呼ばれるものを追加している
- トークンは単語の一部の場合もあって、ChatGPT がときどき「存在しない言葉を造語する」のは、これが理由である
- 普通なら、常に確率の高いトークンが選ばれると思うだろうが、実際は違う
- 毎回最高ランクのトークンばかり選んでいたら、面白みのない単調な論文になってしまう
- そのため、たまにランクの低いトークンをランダムに混ぜ込むことで「もっと興味深い」論文を作成している
  - なんども同じ単語を打ち込むと、たまに違う答えを返す
- ランクの低い単語を使う頻度を決める「温度」というパラメーターが存在し、小論文を生成する場合には、この「温度」を 0.8 に設定すると最もうまく機能することがわかっている
- 確率はどこから求めるのか？
  - 大量の文章の中からそれぞれのアルファベットが登場する確率を出して、それを元に単語を並べる
  - 意味不明な文字の羅列になってしまう..
  - そこで、一つ目の文字に対して、二つ目の文字が出る確率を調べる（a の後に b が来る確率、a の後に c が来る確率...みたいに）
  - q の次に来る文字の確率は低い、などの情報が分かる
  - 文字を並べる際の情報として使える
  - q の後に c は並べない、など
  - 2 文字だけじゃなく、文字数を増やして確率を計算することで、意味のある単語や文を生成することができるようになる
  - 単語でも同じである
  - 1 単語ずつの確率ではなく、2 個の単語あるいは 3 単語以上の単語が続く可能性を考えれば良い
  - だいぶ意味のある単語の羅列になる
  - とはいえ、全ての単語の組み合わせを考えようとしたら、とんでもない確率の組み合わせの数になってしまう
  - 例えば、「小論文の一部」といえる 20 単語までいく頃には、組み合わせの数は宇宙に存在する粒子の数すら超えてしまうので、その全てを書き出すことなど、できるはずがない
  - では、どうするか？
  - 現存するコーパスに含まれるテキストでは単語の組み合わせを実際にはっきりと確認できないとしても、その組み合わせが出現する確率を推定してくれるモデルを作ればいいのではないか。そういう考えが出てくる
  - そして、ChatGPT の中心になっているのが、そのような確率の推定をうまく処理するように設計されたモデル、いわゆる「大規模言語モデル（LLM）」なのである
    - 全ての言葉の組み合わせを考えるのは実際には不可能。だから、モデルは統計的な法則を使って、現実的な言葉の組み合わせを学びます。
- モデルとは何か
  - 落下させる高さによって下に落ちるまでの時間を計算することを考える
  - もちろん実際に試して計算することもできるが、ある程度の数を試せば、計算式を作って、その他の高さから落とした場合の時間も求めることができる
  - この「計算式」のようなものが「モデル」である
  - 「モデルのないモデル」は存在しないということを承知しておくといいだろう
  - どんなモデルでも、その基礎には何らかの構造があり、データに合わせて様々なオプション（設定できるパラメータ）が存在する
  - ChatGPT の場合には、こうしたパラメータが 1750 億個ある
    - ChatGPT もまた特定の構造と多くのパラメータを持つモデルの一例
- 人間と同じような処理をこなすモデル
  - 前説で説明したのは、基本的に単純な物理学に由来する数値データのモデルを作るという例であり、いってみれば「単純な数学が当てはまる」ことが何世紀も前からわかっている例だった
  - それに対して、ChatGPT の場合は、人の脳によって作り出されるのと同じような自然言語の文章に対応するモデルを作らなければならない
  - そのような機能となると、「単純な数学」に当たるようなものを、私たちは（少なくとも）持ち合わせていないのだ
- ニューラルネット
  - 画像認識のような処理をこなす典型的なモデルは、実際にどう動くのか
  - 今のところ最も広く使われ、かつ成功もしているのが、「ニューラルネット」を使うアプローチだ
  - 人間の脳の働きを単純に理想化したものと考えると分かりやすい
  - ニューラルネットはどうやって「ものごとを認識」しているのだろうか
  - ここで鍵となるのが「アトラクター」という概念だ
  -
